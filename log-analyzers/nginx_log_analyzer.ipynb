{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of collecting and analyzing NGINX logs in AWS:\n",
    "\n",
    "1. Download all the logentries from Cloudwatch filtered on `StreamPrefix='application/nginx'`` and analyze them locally\n",
    "2. Execute a query with boto3 and analyze the results\n",
    "\n",
    "This notebook aims to show how to use the second approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install boto3 prettytable pandas --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import time, pytz\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from botocore.exceptions import ClientError\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "os.environ['TZ'] = 'UTC'\n",
    "time.tzset()\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from awsutils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_session(profile_name='acl-production', region='us-east-1'):\n",
    "    global aws_session, sts_client, logs_client\n",
    "\n",
    "    # Use the function with your profile to get a session\n",
    "    aws_session = set_aws_credentials(profile_name, region)\n",
    "\n",
    "    # Create clients using the session\n",
    "    sts_client = aws_session.client('sts')\n",
    "    logs_client = aws_session.client('logs')\n",
    "\n",
    "    # Example usage of the clients\n",
    "    account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "    print(\"Current AWS Account ID:\", account_id)\n",
    "\n",
    "create_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log Collector from AWS CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect logs from the specified log group and log stream into a file\n",
    "\n",
    "log_group = 'projects-main'\n",
    "#log_group = 'reports-main'\n",
    "#log_group = 'results-main'\n",
    "#log_group =  'api_proxy-main'\n",
    "#log_group =  'launchpad-main'\n",
    "#log_group =  'risks-main'\n",
    "\n",
    "\n",
    "log_stream = 'nginx'\n",
    "#log_stream = 'application'\n",
    "\n",
    "start_date = exact_date(2025, 1, 27, 15, 30)\n",
    "end_date = exact_date(2025, 1, 27, 16, 10)\n",
    "\n",
    "\n",
    "base_query = \"fields @timestamp, @message\"\n",
    "base_query += f\"| filter @logStream like 'application/{log_stream}'\"\n",
    "print (f\"query: {base_query}\")\n",
    "\n",
    "\n",
    "# create folder for the log fragments\n",
    "date_str = datetime.fromtimestamp(start_date).strftime(\"%Y%m%d\")\n",
    "folder_name = f'{log_stream}_{log_group}_{date_str}'\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# fetch the log data in slices\n",
    "cloudwatch_crawler(logs_client, log_group, base_query, start_date, end_date, folder_name, slices=1)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompile regular expressions for better performance\n",
    "HTTP_PATTERN = re.compile(r' HTTP/1.1$')\n",
    "ID_PATTERN = re.compile(r'([/=])\\d+')\n",
    "\n",
    "# Create a single mapping for all character replacements\n",
    "CHAR_REPLACEMENTS = {\n",
    "    '%5D': ']',\n",
    "    '%5B': '[',\n",
    "    '%2C': ',',\n",
    "    '%3A': ':',\n",
    "    '%7B': '{',\n",
    "    '%7D': '}',\n",
    "    '%22': '\"',\n",
    "    '%20': ' ',\n",
    "    '%3F': '?',\n",
    "    '%3D': '=',\n",
    "    '%26': '&',\n",
    "    '%25': '%',\n",
    "    '%2F': '/',\n",
    "    '%5C': '\\\\',\n",
    "}\n",
    "\n",
    "# Create escape pattern at module level\n",
    "ESCAPE_PATTERN = re.compile('|'.join(map(re.escape, CHAR_REPLACEMENTS.keys())))\n",
    "\n",
    "def normalize_request_path(request):\n",
    "    # Remove \"HTTP/1.1\" if it exists at the end using precompiled pattern\n",
    "    request = HTTP_PATTERN.sub('', request)\n",
    "    \n",
    "    # Replace all escaped characters in one pass\n",
    "    request = ESCAPE_PATTERN.sub(\n",
    "        lambda m: CHAR_REPLACEMENTS[m.group(0)], \n",
    "        request\n",
    "    )\n",
    "\n",
    "    # Replace numeric IDs using precompiled pattern\n",
    "    request = ID_PATTERN.sub(r'\\1{id}', request)\n",
    "\n",
    "    return request\n",
    "\n",
    "def aggregate_requests(aggregated_data, data):\n",
    "    for entry in data:\n",
    "        try:\n",
    "            request = entry[\"@message\"][\"request\"]\n",
    "            normalized_request = normalize_request_path(request)\n",
    "            request_time = int(float(entry[\"@message\"][\"request_time\"]) * 1000)  # Convert to milliseconds\n",
    "\n",
    "            if normalized_request in aggregated_data:\n",
    "                aggregated_data[normalized_request]['count'] += 1\n",
    "                aggregated_data[normalized_request]['total_duration'] += request_time\n",
    "            else:\n",
    "                aggregated_data[normalized_request] = {'count': 1, 'total_duration': request_time}\n",
    "        except KeyError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(\"Error processing entry:\", entry)\n",
    "            print(e)\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the json file with log entries into a list\n",
    "aggregated_data = {}\n",
    "\n",
    "folder_name = 'nginx_projects-main_20250128'\n",
    "log_files = [os.path.join(folder_name, f) for f in os.listdir(folder_name) if f.endswith('.json')]\n",
    "\n",
    "item_count=0\n",
    "        \n",
    "for fname in log_files:\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            item_count += len(data)\n",
    "            print(f'\\rnumber of log entries: {item_count}', end=\"\")\n",
    "            parse_json_messages(data)\n",
    "            aggregate_requests(aggregated_data, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print aggregated data in a more readable format\n",
    "print(\"\\nAggregated Request Statistics:\")\n",
    "for path, stats in aggregated_data.items():\n",
    "    avg_duration = stats['total_duration'] / stats['count']\n",
    "    print(f\"\\nPath: {path}\")\n",
    "    print(f\"Count: {stats['count']}\")\n",
    "    print(f\"Total Duration (ms): {stats['total_duration']}\")\n",
    "    print(f\"Average Duration (ms): {avg_duration:.2f}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AWS-utils",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
